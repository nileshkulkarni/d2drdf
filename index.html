<!DOCTYPE html>
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
   
</style>
<html>

<head>
    <link rel="stylesheet" type="text/css" href="style.css">
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-K8GV7XKZ6M"></script>
    <!-- Google tag (gtag.js) -->
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-K8GV7XKZ6M'); 
    </script>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-K8GV7XKZ6M"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-K8GV7XKZ6M');
    </script>
    <script src=”http://code.jquery.com/jquery-1.9.1.js”></script>
    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
    <script src="https://d3js.org/d3.v6.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML" async></script>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/noUiSlider/15.4.0/nouislider.min.css" />
    <title>Learning to Predict Scene-Level Implicit 3D from Posed RGBD data</title>

    <meta http-equiv="content-type" content="text/html; charset=UTF8">
    <meta property="og:title" content="D2DRDF" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta property="og:image" content="https://nileshkulkarni.github.io/d2drdf/assets/images/fig4.png" />
    <meta property="og:url" content="https://nileshkulkarni.github.io/d2drdf/" />
    <meta property="og:video" content="https://nileshkulkarni.github.io/d2drdf/assets/videos/matterport_novel_1.mp4" />
    <!-- <meta property="twitter:card" content="summary_large_image"> -->
    <meta property="twitter:card" content="player" />
    <meta property="twitter:description" content="Learnign to Predict Scene Level Implicit 3D from Posed RGBD Data" />
    <!-- <meta property="twitter:image" content="https://nileshkulkarni.github.io/d2drdf/assets/gifs/8WUmhLawc2A_aaab09df2ec34ea584e93e42ad0cf8e4_i1_0.gif" /> -->
    <meta property="twitter:player" content="https://nileshkulkarni.github.io/d2drdf/assets/videos/matterport_novel_1.mp4" />
    <meta propery="twitter:player:width" content="400" />
    <meta name="twitter:player:width" content="400" />
    <meta propery="twitter:player:height" content="400" />
    <meta name="twitter:player:height" content="400" />
    <meta property="twitter:title" content="D2-DRDF" ></meta>
    <meta property="og:description" content="N. Kulkarni, L. Jin, J. Johnson, D. Fouhey." />
    <!-- <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet"
          integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous"> -->
    <!-- <link href="./bootstrap/css/bootstrap.min.css" rel="stylesheet"> -->



</head>

<body>
    <br>
    <table width=1000px>
        <tr>
            <td>
                <center>
                    <span style="font-size:42px">Learning to predict Scene Level  </span>
                </center>
            </td>
        </tr>
        <tr>
            <td>
                <center>
                    <span style="font-size:42px">Implicit 3D from Posed RGBD data</span>
                </center>
            </td>
        </tr>
    </table>

    <br><br>
    <table align=center width=1000px>
        <tr>
            <td align=center width=100px>
                <center>
                    <span style="font-size:20px"><a href="https://nileshkulkarni.github.io/">Nilesh Kulkarni</a></span>
                </center>
            </td>
            <td align=center width=100px>
                <center>
                    <span style="font-size:20px"><a href="https://jinlinyi.github.io/">Linyi Jin</a></span>
                </center>
            </td>
            <td align=center width=100px>
                <center>
                    <span style="font-size:20px"><a href="https://web.eecs.umich.edu/~justincj/">Justin
                            Johnson</a></span>
                </center>
            </td>
            <td align=center width=100px>
                <center>
                    <span style="font-size:20px"><a href="https://web.eecs.umich.edu/~fouhey/">David F.
                            Fouhey</a></span>
                </center>
            </td>

        </tr>
    </table>

    <br>
    <table align=center width=1000px>
        <tr>
            <td align=center width=100px>
                <center>
                    <span style="font-size:20px">University of Michigan, Ann Arbor</span>
                </center>
            </td>
        </tr>
    </table>

    <br>
    <table align=center width=400px>
        <tr>
            <td align=center width=100px>
                <center>
                    <span style="font-size:20px">CVPR 2023</span>
                </center>
            </td>
        </tr>
    </table>

    <br>
    <table width=1000px>
        <tr>
            <td>
                <table align=center width=300px>
                    <tr>
                        <td align=center width=100px>
                            <center>
                                <span style="font-size:20px"><a href="./assets/paper.pdf">[pdf]</a></span>
                            </center>
                        </td>
                        <!-- <td align=center width=100px>
                <center>
                    <span style="font-size:20px"><a href="./assets/supp.pdf">[supp]</a></span>
                </center>
            </td> -->
                        <td align=center width=100px>
                            <center>
                                <span style="font-size:20px"><a
                                        href="http://www.github.com/nileshkulkarni/d2drdf/">[code]</a></span>
                            </center>
                        </td>
                        <td align=center width=100px>
                            <center>
                                <span style="font-size:20px"><a href="https://www.youtube.com/watch?v=cp7iMF9JWHM">[video]</a></span>
                            </center>
                        </td>
                        <!-- <td align=center width=100px>
        <center>
        <span style="font-size:20px"><a href="https://www.youtube.com/watch?v=93M3ou4mg-w&feature=youtu.be">[video]</a></span>
        </center>
        </td> -->

                    </tr>
                </table>
            </td>
        </tr>
    </table>
    <table align=center width=400px>
        <tr>
            <td align=center width=100px>
                <center>
                    <span style="font-size:20px"></span>
                </center>
            </td>
        </tr>
    </table>

    <br>
    <table align=center width=1000px>
        <tr>
            <td>
                <!-- <center>
                    <span style="font-size:22px">Our approach generates 3D from a single unseen image (left) while
                            training on real 3D scans. We show the two rendered novel views of our output (middle,
                            right). Visible surfaces are colored by the image and occluded ones are colored with surface
                            normals: pink is upwards and lavender faces the camera. We can observe the occluded cabinet
                            and floors behind the couch.
                </center> -->
                <div class="text_div" style="font-size:22px">
                   <!-- <span> <b> TL;DR</b> A model to predict single image to 3D implicit function. D2-DRDF does not require mesh supervision at training and can directly work with captured raw RGB-D data. </span> -->
                   <span> <b> TL;DR</b> We use a D2-DRDF model to predict a 3D implicit function from a single input image. Unlike other methods, D2-DRDF does not depend on mesh supervision during training and can directly operate with raw RGB-D data obtained from scene captures. </span>
                </div>
            </td>
        </tr>
    </table>
    <br>
    <table align="center" width=1000px>
        <tr>
            <td class="td-center">
                <span style="font-size:22px">  </span>
            </td>
        </tr>
    </table>
    <table align="center" width=1000px>
        <tr>
            <td>

                <div class="container">
                <video id="glass" width="500" autoplay muted loop>
                    <source src="assets/videos/matterport_novel_1.mp4" type="video/mp4">
                </video>
            </div>
            </td>
            <td>
                <div class="container">
                    <video id="glass" width="500" autoplay muted loop>
                        <source src="assets/videos/matterport_novel_2.mp4" type="video/mp4">
                    </video>
                </div>
            </td>
        </tr>
        <tr>
            <div height=40></div>
        </tr>
        <tr>
            <td>

                <div class="container">
                <video id="glass" width="500" autoplay muted loop>
                    <source src="assets/videos/omnidata_novel_1.mp4" type="video/mp4">
                </video>
            </div>
            </td>
            <td>
                <div class="container">
                    <video id="glass" width="500" autoplay muted loop>
                        <source src="assets/videos/omnidata_novel_2.mp4" type="video/mp4">
                    </video>
                </div>
            </td>
        </tr>
    </table>
    <table align="center" width=1000px>
        <tr><td style="font-size: 22px;"><div class="text_div"> <b> Sample results obtained from previously unseen images sourced from the Matterport3D (top) and OmniData (bottom) datasets.</b> D2-DRDF exhibits the capability to reconstruct hidden sections of the floor and the rear portion of the couch. 
        <!-- Impressively, it can even reconstruct entire empty rooms located behind walls. -->
        </div></td></tr>
    </table>
    <hr>
    <br>
    <table align=center width=1000px>
        <tr>
        <td class="td-center">
            <h1> Abstract</h1>
        </td>
        </tr>
        <tr>
            <td>
                <div class="text_div" style="font-size:22px">
                    We introduce a method that can learn to predict scene-level implicit functions for 3D reconstruction from posed RGBD data. 
                    At test time, our system maps a previously unseen RGB image to a 3D reconstruction of a scene via implicit functions. 
                    While implicit functions for 3D reconstruction have often been tied to meshes, we show that we can train one using only a set of posed RGBD images. 
                    This setting may help 3D reconstruction unlock the sea of accelerometer+RGBD data that is coming with new phones. 
                    Our system, D2-DRDF, can match and sometimes outperform current methods that use mesh supervision and shows better robustness to sparse data.
                </div>
            </td>
        </tr>
    </table>
    <table align=center width=1000px>
        <tr>
            <td class="td-center">
                <h1> Overview</h1>
            </td>
        </tr>
        <tr>
            <td>
                <div class="text_div" style="font-size:22px">
                    In this paper, we propose a method for reconstructing 3D scenes, including occluded regions, based on RGB images that have not been previously seen. To train our approach, we utilize posed RGB and depth data. Our model represents the 3D scene using an the <a href="https://nileshkulkarni.github.io/scene_drdf"> Directed Ray Distance Function (DRDF)</a>.

                    In this work, we demonstrate how to geometrically supervise this DRDF function by leveraging partial observations obtained from auxiliary views. We summarize our key insights as follows:
                    <ul>
                        <li> 
                            <!-- Auxilary views observe occluded regions corresponding to a ray in reference view. These observations point to regions along the ray that correspond to free space. -->
                            Auxiliary views observe parts of the ray that are occluded in the reference view.  These free space observations along the ray provide independent information for different segments of the ray.
                        </li>
                        <li>
                            Combining segment level information acros auxiliary views for a ray aids in supervising the distinct segments along the ray. This is accomplished through the application of equality and inequality constraints on the predicted values of DRDF.
                        </li>
                    </ul>
                </div>
            </td>
        </tr>
    </table>
    <br><br>
    <hr>
    <center>
        <table align=center width=1000px>
            <!-- <h1>Approach</h1> -->
            <tr>
                <td class="td-center">
                    <h1>Approach</h1>
                </td>
            </tr>
            <tr>
                <td class="td-center"> <div class="text_div" style="font-size:22px">  We present an approach to train a model to predict 3D from single images. 
                    Our model is supervised with Posed RGBD data, below we highlight the key ideas.
                    </div> </td>
            </tr>
        </table>
        <div class="text_div">
        </div>
        <div>
            <table width="1000px">
                <tr>
                    <td style="padding:25px">
                        <img src="assets/images/fig4.png" width="500px" />
                    </td>
                    <td style="padding:25px">
                        <div class="text_div" style="font-size:22px"><b>Learning from Auxiliary Views.</b>
                            For each <span style="color:red;">red ray</span> originating from the reference camera (R), we extract depth information from an auxiliary image view for points along the ray. Views (a), (b), and (c) on the right capture distinct occluded segments along the ray, providing valuable free-space information. This information enables the creation of penalty functions to train the DRDF function.
                        </div>
                    </td>
                </tr>
                <tr>
                    <td style="padding:25px">
                        <img src="assets/images/fig3.png" width="500px" />
                    </td>
                    <td style="padding:25px">
                        <div class="text_div" style="font-size:22px"> <b> Segment Types.</b> 
                        When the ray from the reference camera is seen by an auxiliary views, there are segments of freespace. Depending on how these segments start and end, they place different constraints on the DRDF. Here we show a segment that <b>s</b>tarts with a disocclusion and <b>e</b>nds with an intersection. The space between the <b>s</b> and <b>e</b> events is unoccupied and we convert this information to a penalty function that is used to train the model.
                        <br> <br> 
                        We show an interactive demo of this penalty plot in the <a href="#interactive-demo"> next section</a> for different segment types</div>.
                        <!-- Every auxiliary view that observes the a ray creates segments along the ray. -->
                        <!-- In an auxiliary view we can observe the ray entering the view and exiting it. These events happen because the ray originating from the camera either intersects with the scene geometry or under goes an (dis)occlusion  event. We show in the image on the right one such segment <b>(s,e)</b>. In this case <b> s </b> is an dis-occlusion event and e is an interaction event. Every segment creates a different penalty regions that can used to train the DRDF function. We show an interactive demo of this in the <a href="#interactive-demo"> next section</a></div> -->
                    </td>
                </tr>
                <tr>
                    <td style="padding:25px">
                        <img src="assets/images/fig2.png" width="500px" />
                    </td>
                    <td style="padding:25px">
                        <!-- <div class="text_div" style="font-size:22px"> <b> Method Overview.</b> At training time, our model takes an image and set of 3D points along rays. These points along the ray are supervised with  <a href="#interactive-demo">penalty functions</a> for different segments along the ray. We aggregate the information across different auxilary views to create a consistent list of segment information along the ray. We require our network to predict the <a href="https://nileshkulkarni.github.io/scene_drdf"> DRDF </a> function for points in the scene. At inference we predict the DRDF for all points in the image frustum and decode it to a scene-gemoetry.</div> -->
                        <div class="text_div" style="font-size:22px"> <b>Method Overview.</b> During the training process, or when considering a specific ray from the reference view, we utilize auxiliary views to determine the free-space segments along the ray. Subsequently, for each 3D point on this ray, we employ our network to predict the DRDF value and calculate the associated penalty. During inference, our network is tasked with predicting the DRDF function for points within the image frustum of a single image. For more information on the DRDF function, you can visit this <a href="https://nileshkulkarni.github.io/scene_drdf">link</a>.
                    </td>
                </tr>

            </table>
        </div>
    </center>
    <br>
    <hr>
    <div>
        <div id="interactive-demo">
        <!-- <center> -->
        <table width="1000px">
            <tr>
                <td class="td-center">
                    <h1> Interactive Demo for Segment Penalty Plots</h1>
                </td>
            </tr>
            <tr>
                <td>
                    We demonstrate the influence of different segments along the ray on the imposition of distinct penalty functions for predicted DRDF valuesn show below. On the X-axis, we interactively select the positions of Intersection (I) and Occlusion (O) events along the ray. The placement of these events determines which DRDF functions are penalized as inconsistent with the observed intersection or occlusion. We generate a heat map to represent the penalty associated with the DRDF value (Y-axis) for a specific point along the ray (X-axis). Regions depicted in <span style="color:red;">dark red</span> indicate high penalty magnitude, while light grey regions indicate low penalty. 
                    <br><br>
                    The key property of the DRDF function is for any point along the ray,  \(z \in [0, Z]\), if \(DRDF(z)= d\) then there is an intersection at point \( z + d \) on the ray. All the penalty segments below are the implicition of this equation.
                    
            </tr>
        </table>
        <!-- </center> -->
        </div>
    
        <div >
        <table width="1100px">
            <tr>
                <td>
                <div id='penalty-ii'>
                </div>
                </td>
                <td>
                    <div id='penalty-oo'>
                    </div>
                </td>
            </tr>
            <tr>
            <td>
            <div class="slider-container">
                <table>
                <tr><td>
                   <div id="slider-ii" style="width: 500px"></div> 
                </td></tr>
                <tr><td>
                <div id="slider-label1-ii" class="slider-label" style="width: 500px">
                      <div id="slider-label1-text-ii"> </div>
                </div>
                <div id="slider-label2-ii" class="slider-label" style="width: 500px">
                    <div id="slider-label2-text-ii"> </div>
                </div>
                </td></tr>
                <tr>
                    <td><div style="height:10px"></div></td>
                </tr>
                <tr>
                    <td class="td-center">
                        
                        <b  style="font-size:22pt">(a)</b>
                   
                    </td class="td-center">
                </tr>
                <tr>
                    <td class="td-center">
                        <div style="width: 500px" > 
                        You can observe that this plot represents the penalty function for a segment between two intersection events (II). It is clear that the only DRDF values that meet the criteria align slopped at -1, this specific line depicted as brighter regions in the plot with zero penalty. Shifting the slider horizontally causes this line to move in parallel along the X-axis. This particular segment, denoted as II, imposes an equality constraint.
                    </div>
                    </td class="td-center">
                </tr>
                </table>
            </div>
            </td>
            <td>
            <div class="slider-container">
                <table>
                <tr><td>
                    <div id="slider-oo" style="width: 500px"></div> 
                </td></tr>
                <tr><td>
                <div id="slider-label1-oo" class="slider-label" style="width: 500px">
                        <div id="slider-label1-text-oo"> </div>
                </div>
                <div id="slider-label2-oo" class="slider-label" style="width: 500px">
                    <div id="slider-label2-text-oo"> </div>
                </div>
                </td></tr>
                <tr>
                    <td><div style="height:10px"></div></td>
                </tr>
                <tr>
                    <td class="td-center">
                        <b  style="font-size:22pt">(b)</b>
                    </td class="td-center">
                </tr>
                <tr>
                    <td class="td-center">
                        <div style="width: 500px" >
                        You can observe that this plot represents ray segment between two occlusion events (OO).
                        The plot illustrates that DRDF values cannot fall within the red strip, establishing an inequality constraint that permits DRDF to take values outside of this region. By adjusting the positions of the <b>O</b> events, the width of the red strip can be increased or decreased.
                    </div>
                    </td class="td-center">
                </tr>
                </table>
            </div>
            </td>
        </tr>
            <tr>
                <td><div style="height:50px"></div></td>
            </tr>
        </table>
        <hr>
        <table width="1100px">
            <tr>
                <td>
                <div id='penalty-io'>
                </div>
                </td>
                <td>
                    <div id='penalty-oi'>
                    </div>
                </td>
            </tr>
            <tr>
            <td>
            <div class="slider-container">
                <table>
                <tr><td>
                   <div id="slider-io" style="width: 500px"></div> 
                </td></tr>
                <tr><td>
                <div id="slider-label1-io" class="slider-label" style="width: 500px">
                      <div id="slider-label1-text-io"> </div>
                </div>
                <div id="slider-label2-io" class="slider-label" style="width: 500px">
                    <div id="slider-label2-text-io"> </div>
                </div>
                </td></tr>
                <tr>
                    <td><div style="height:10px"></div></td>
                </tr>
                <tr>
                    <td class="td-center">
                        <b  style="font-size:22pt">(c)</b>
                    </td class="td-center">
                </tr>
                <tr>
                    <td class="td-center">
                        <div style="width: 500px" >
                        This scenario encompasses events from both (a) and (b). Points located closer to the I event are limited to a single DRDF value. However, points closer to the O event must adhere to DRDF values that prevent any additional intersections within the <b>IO</b> segment. This segment introduces both an equality and an inequality constraint.
                    </div>
                    </td class="td-center">
                </tr>
                </table>
            </div>
            </td>
            <td>
            <div class="slider-container">
                <table>
                <tr><td>
                    <div id="slider-oi" style="width: 500px"></div> 
                </td></tr>
                <tr><td>
                <div id="slider-label1-oi" class="slider-label" style="width: 500px">
                        <div id="slider-label1-text-oi"> </div>
                </div>
                <div id="slider-label2-oi" class="slider-label" style="width: 500px">
                    <div id="slider-label2-text-oi"> </div>
                </div>
                </td></tr>
                <tr>
                    <td><div style="height:10px"></div></td>
                </tr>
                <tr>
                    <td class="td-center">
                        <b  style="font-size:22pt">(d)</b>
                    </td class="td-center">
                </tr>
                <tr>
                    <td class="td-center">
                        <div style="width: 500px" >
                        This scenario combines events from both (a) and (b). Points along the ray closer to the I event are constrained to a single DRDF value. On the other hand, points closer to the O event must assume DRDF values that prevent any additional intersections within the <b>OI</b> segment. Consequently, this segment introduces both an equality and an inequality constraint.
                    </div>
                    </td class="td-center">
                </tr>
                </table>
            </div>
            </td>
        <tr>
            <td><div style="height:50px"></div></td>
        </tr>

        </tr>
        </table>
        </div>
        </div>
    <hr>
    <center>
        <table align=center width=1000px>
            <!-- <h1>Approach</h1> -->
            <tr>
                <td class="td-center">
                    <h1> Overview Video</h1>
                </td>
            </tr>
        </table>
        <div width=1000px>
            <center>
            <table align=center width="1000px">
                <tr>
                    <td>
                        <iframe width="1000" height="461" src="https://www.youtube.com/embed/cp7iMF9JWHM" title="D2DRDF Overview (CVPR 23)" frameborder="0" 
                                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen>
                        </iframe>
                    </td>
                </tr>
            </table>
            </center>
        </div>
    </center>
    <br>
    <hr>


    <table align=center width=1000px>
        <!-- <h1>Approach</h1> -->
        <tr>
            <td class="td-center">
                <h1> Paper</h1>
            </td>
        </tr>
    </table>
    <table align=center width=1000px>
        <tr>
            <td>
                <table align=center width=650px>
                    <tr>
                        <td style="padding:25px"><a href="https://arxiv.org/"><img  class="layered-paper-big" style="height:250px"
                                    src="./assets/paper.png" /></a></td>
                        <td style="padding:25px"><span style="font-size:14pt">Kulkarni, Jin, Johnson, Fouhey<br><br>
                                Learning to Predict Scene Level Implicit 3D from Posed RGBD data<br><br>
                                CVPR, 2023
                                <br>
                                <!-- [hosted on <a href="#">arXiv</a>]</a> -->
                        </td>
                    </tr>
                </table>
            </td>
        </tr>
    </table>
    <br>
    <table align=center width=1000>
        <tr>
            <td>
                <table align=center width=180px>
                    <tr>
                        <td><span style="font-size:14pt">
                                <center>
                                    <a href="assets/paper.pdf">[pdf]</a>
                                </center>
                        </td>
                        <!-- <td><span style="font-size:14pt">
                    <center>
                        <a href="./assets/supp.pdf">[supp]</a>
                    </center>
                </td> -->

                        <td><span style="font-size:14pt">
                                <center>
                                    <a href="./assets/bibtex.txt">[bibtex]</a>
                                </center>
                        </td>
                    </tr>
                </table>
            </td>
        </tr>
    </table>
    <br>

    <hr>


    <table align=center width=1000px>
        <!-- <h1>Approach</h1> -->
        <tr>
            <td class="td-center">
                <h1> Code</h1>
                Coming soon...
            </td>
        </tr>
    </table>

    <table align=center width=1000px>
        <tr>
            <td>
                <center> <br>
                    <span style="font-size:28px">&nbsp;<a
                            href='https://github.com/nileshkulkarni/d2drdf'>[GitHub]</a>

                        <span style="font-size:28px"></a></span>
                        <br>
                </center>
            </td>
        </tr>
    </table>
    <br>
    <hr>

    <table align=center width=1000px>
        <tr>
            <td>
                <left>
                    <center>
                        <h1>Acknowledgements</h1>
                    </center>
                    We thank our colleagues for the wonderful discussions on the project (in alphabetical order). Richard Higgins, Sarah Jabour, Dandan Shan, Karan Desai, Mohamed El Banani,
and Chris Rockwell for feedback. We acknowledge Shengyi Qian's help
with <a href="https://github.com/facebookresearch/viewseg">ViewSeg </a> code used to implement the PixelNeRF baseline.
Toyota Research Institute (“TRI”) provided funds to assist the authors
with their research but this article solely reflects the opinions
and conclusions of its authors and not TRI or any Toyota entity.
                    NK was supported by TRI. Toyota Research Institute
                    (''TRI'') provided funds to assist the authors with their research but this article solely reflects
                    the opinions and conclusions of its authors and not TRI or any other Toyota entity. This base
                    version
                    of the template is borrowed from <a href="https://richzhang.github.io/colorization/">colorful
                        folks</a>.
                </left>
            </td>
        </tr>
    </table>

    <br><br>
    <script src="./density.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/noUiSlider/15.4.0/nouislider.min.js"></script>
    <!-- <div id="graph"></div> -->
    <script>
        // draw_slider()
        // draw_slider_animate()
        setup_slider('ii')
        updateSliderValues(1.0, 2.0,'ii')
        setup_slider('oo')
        updateSliderValues(1.0, 2.0,'oo')
        setup_slider('oi')
        updateSliderValues(1.0, 2.0,'oi')
        setup_slider('io')
        updateSliderValues(1.0, 2.0,'io')
        // updateSliderValues(1.1, 2.1)
        // console.log('DRDF Penalty')
        // console.log(iiValue(1,0, 1, 2))
        // console.log(gtDRDF(1, 1, 2))
        
        // draw_example()
    </script>
</body>

</html>